{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\GzuzC\\OneDrive\\Documents\\Notes\\Personal Studies\\Workbooks and Worksheets\\Python\\PerScholas Data Engineering\\perscholas_capstone_project\\ignore\\ETL-capstone-env\\lib\\site-packages\\pyspark\\pandas\\__init__.py:49: UserWarning: 'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. pandas-on-Spark will set it for you but it does not work if there is a Spark context already launched.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pyspark import SparkContext\n",
    "import pyspark.pandas as ps\n",
    "from pyspark.sql import SparkSession\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attempt at extracting local JSON file without using pyspark module \n",
    "\n",
    "# pyspark module seems un-nessisary for this part, plus using SQL queries for\n",
    "# transformations would be a huge pain in the ass. \n",
    "\n",
    "sc = SparkContext(appName=\"Spark-Test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-AQ9UQ3O:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Spark-Test</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=Spark-Test>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_customer = sc.textFile(\"../data/credit_card_dataset/cdw_sapp_custmer.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.RDD'>\n",
      "['{\"FIRST_NAME\":\"Alec\",\"MIDDLE_NAME\":\"Wm\",\"LAST_NAME\":\"Hooper\",\"SSN\":123456100,\"CREDIT_CARD_NO\":\"4210653310061055\",\"APT_NO\":\"656\",\"STREET_NAME\":\"Main Street North\",\"CUST_CITY\":\"Natchez\",\"CUST_STATE\":\"MS\",\"CUST_COUNTRY\":\"United States\",\"CUST_ZIP\":\"39120\",\"CUST_PHONE\":1237818,\"CUST_EMAIL\":\"AHooper@example.com\",\"LAST_UPDATED\":\"2018-04-21T12:49:02.000-04:00\"}']\n",
      "<class 'list'>\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "# type of return with SparkContext.textFile method\n",
    "print(type(rdd_customer)) \n",
    " \n",
    "\n",
    "zeroth_item = rdd_customer.take(1)\n",
    "# contents of take.(1)\n",
    "print(zeroth_item)\n",
    "\n",
    "# take produces list\n",
    "print(type(zeroth_item))\n",
    "\n",
    "# returns string\n",
    "print(type(zeroth_item[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_customer_json = rdd_customer.map(lambda json_item: json.loads(json_item) )  # similar to apply in pandas or just regular map func in python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.PipelinedRDD'> \n",
      "\n",
      "[{'FIRST_NAME': 'Alec', 'MIDDLE_NAME': 'Wm', 'LAST_NAME': 'Hooper', 'SSN': 123456100, 'CREDIT_CARD_NO': '4210653310061055', 'APT_NO': '656', 'STREET_NAME': 'Main Street North', 'CUST_CITY': 'Natchez', 'CUST_STATE': 'MS', 'CUST_COUNTRY': 'United States', 'CUST_ZIP': '39120', 'CUST_PHONE': 1237818, 'CUST_EMAIL': 'AHooper@example.com', 'LAST_UPDATED': '2018-04-21T12:49:02.000-04:00'}] \n",
      "\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "print(type(rdd_customer_json), \"\\n\")\n",
    "\n",
    "zeroth_item_json = rdd_customer_json.take(1)\n",
    "print(zeroth_item_json, \"\\n\")\n",
    "\n",
    "print(type(zeroth_item_json[0][\"CUST_ZIP\"]))  # converted to json-like dict but all values are still strings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attempt at using local json file as basis for df with pyspark.sql\n",
    "pyspark = SparkSession.builder.appName(\"Spark-SQL-Test\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_df = pyspark.read.json(\"../data/credit_card_dataset/cdw_sapp_custmer.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'int'>\n"
     ]
    }
   ],
   "source": [
    "print(type(zeroth_item_json[0][\"CUST_PHONE\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- APT_NO: string (nullable = true)\n",
      " |-- CREDIT_CARD_NO: string (nullable = true)\n",
      " |-- CUST_CITY: string (nullable = true)\n",
      " |-- CUST_COUNTRY: string (nullable = true)\n",
      " |-- CUST_EMAIL: string (nullable = true)\n",
      " |-- CUST_PHONE: long (nullable = true)\n",
      " |-- CUST_STATE: string (nullable = true)\n",
      " |-- CUST_ZIP: string (nullable = true)\n",
      " |-- FIRST_NAME: string (nullable = true)\n",
      " |-- LAST_NAME: string (nullable = true)\n",
      " |-- LAST_UPDATED: string (nullable = true)\n",
      " |-- MIDDLE_NAME: string (nullable = true)\n",
      " |-- SSN: long (nullable = true)\n",
      " |-- STREET_NAME: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customer_df.printSchema() \n",
    "# both vanilla pyspark and pyspark.sql produce basically the same results,\n",
    "# with the same default types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_df = rdd_customer_json.toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------------+------------+-------------+--------------------+----------+----------+--------+----------+---------+--------------------+-----------+---------+-----------------+\n",
      "|APT_NO|  CREDIT_CARD_NO|   CUST_CITY| CUST_COUNTRY|          CUST_EMAIL|CUST_PHONE|CUST_STATE|CUST_ZIP|FIRST_NAME|LAST_NAME|        LAST_UPDATED|MIDDLE_NAME|      SSN|      STREET_NAME|\n",
      "+------+----------------+------------+-------------+--------------------+----------+----------+--------+----------+---------+--------------------+-----------+---------+-----------------+\n",
      "|   656|4210653310061055|     Natchez|United States| AHooper@example.com|   1237818|        MS|   39120|      Alec|   Hooper|2018-04-21T12:49:...|         Wm|123456100|Main Street North|\n",
      "|   829|4210653310102868|Wethersfield|United States| EHolman@example.com|   1238933|        CT|   06109|      Etta|   Holman|2018-04-21T12:49:...|    Brendan|123453023|    Redwood Drive|\n",
      "|   683|4210653310116272|     Huntley|United States| WDunham@example.com|   1243018|        IL|   60142|    Wilber|   Dunham|2018-04-21T12:49:...|   Ezequiel|123454487| 12th Street East|\n",
      "|   253|4210653310195948|   NewBerlin|United States|  EHardy@example.com|   1243215|        WI|   53151|   Eugenio|    Hardy|2018-04-21T12:49:...|      Trina|123459758|Country Club Road|\n",
      "|   301|4210653310356919|      ElPaso|United States|  WAyers@example.com|   1242074|        TX|   79930|   Wilfred|    Ayers|2018-04-21T12:49:...|        May|123454431|   Madison Street|\n",
      "|     3|4210653310395982|NorthOlmsted|United States|BWoodard@example.com|   1242570|        OH|   44070|      Beau|  Woodard|2018-04-21T12:49:...|    Ambrose|123454202|   Colonial Drive|\n",
      "|    84|4210653310400536|      Vienna|United States|   SKemp@example.com|   1239685|        VA|   22180|    Sheila|     Kemp|2018-04-21T12:49:...|      Larry|123451799|   Belmont Avenue|\n",
      "|   728|4210653310459911|      Duarte|United States| WHurley@example.com|   1238213|        CA|   91010|     Wendy|   Hurley|2018-04-21T12:49:...|        Ora|123453875|     Oxford Court|\n",
      "|    81|4210653310773972|      Owosso|United States|AGilmore@example.com|   1240689|        MI|   48867|      Alec|  Gilmore|2018-04-21T12:49:...|     Tracie|123457511|    Forest Street|\n",
      "|   561|4210653310794854|        Zion|United States|    BLau@example.com|   1235222|        IL|   60099|    Barbra|      Lau|2018-04-21T12:49:...|    Mitchel|123457464|     Court Street|\n",
      "|   622|4210653310817373|  Youngstown|United States|EThomson@example.com|   1241363|        OH|   44512|   Edmundo|  Thomson|2018-04-21T12:49:...|      Denny|123457639|    Cypress Court|\n",
      "|   924|4210653310844617| Summerville|United States| ETruong@example.com|   1236228|        SC|   29483|      Elsa|   Truong|2018-04-21T12:49:...|   Isabelle|123453242|  8th Street West|\n",
      "|   611|4210653311015303|      ElPaso|United States|HMckinney@example...|   1238165|        TX|   79930|     Homer| Mckinney|2018-04-21T12:49:...|      Henry|123454339|      East Avenue|\n",
      "|   680|4210653311215039|      Fenton|United States|   RKidd@example.com|   1234730|        MI|   48430|      Rita|     Kidd|2018-04-21T12:49:...|     Rickey|123454537|         Route 44|\n",
      "|    71|4210653311229354|  Grandville|United States|ABallard@example.com|   1242113|        MI|   49418|    Amalia|  Ballard|2018-04-21T12:49:...|  Heriberto|123452373|    Warren Street|\n",
      "|   195|4210653311652836|    YubaCity|United States| PThomas@example.com|   1239888|        CA|   95993|     Patty|   Thomas|2018-04-21T12:49:...|   Angelita|123455343|     Jones Street|\n",
      "|   500|4210653311707126|   CapeCoral|United States| JMorrow@example.com|   1240158|        FL|   33904|  Josefina|   Morrow|2018-04-21T12:49:...|   Dorothea|123451533|       New Street|\n",
      "|   989|4210653311730764|  Brookfield|United States|NAndrews@example.com|   1241408|        WI|   53045|    Nelson|  Andrews|2018-04-21T12:49:...|  Jefferson|123459278|  Division Street|\n",
      "|   810|4210653311898082|    Richmond|United States|MSchneider@exampl...|   1238390|        VA|   23223|    Miquel|Schneider|2018-04-21T12:49:...|     Maximo|123456915|     Maple Street|\n",
      "|   649|4210653312021765| WestChester|United States|PTidwell@example.com|   1235067|        PA|   19380|    Parker|  Tidwell|2018-04-21T12:49:...|    Arnulfo|123453807|       Eagle Road|\n",
      "+------+----------------+------------+-------------+--------------------+----------+----------+--------+----------+---------+--------------------+-----------+---------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rdd_df.show()  # rdd translates into a df nicely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ETL-capstone-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ee7ea43159b5ae9688b9ea80145f8aa5b0cd36336747c8056b8bd8bf647002b8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
